Здесь я использую подход, который начинается с обработки аудиофайлов в формате `.wav`. С помощью библиотеки `librosa` я загружаю аудио, приводя его к частоте дискретизации 16,000 Гц. Если длина аудио меньше минимального значения (1 секунда), я дополняю его нулями, чтобы оно соответствовало необходимой длине. После этого аудио преобразуется в мел-спектрограмму, которая представляет звуковую информацию в частотно-временном формате. Этот процесс включает параметры, такие как количество мел-фильтров (например, 128), размер окна преобразования Фурье и шаг окна, чтобы обеспечить детализацию спектрограммы. Затем спектрограмма нормализуется и преобразуется в тензор, который будет передан в модель. Для обучения все спектрограммы дополнительно приводятся к фиксированной длине: если спектрограмма короче, она дополняется нулями; если длиннее, обрезается. Это гарантирует, что входы модели имеют одинаковый размер.

Далее данные поступают в модель `TransformerClassifier`. На первом этапе используется линейная проекция, которая преобразует спектрограмму из размерности `[количество временных шагов, количество мел-фильтров]` в пространство с фиксированной размерностью `d_model` (например, 256). Затем позиционное кодирование добавляет синусоидальные и косинусоидальные значения к каждому временному шагу, чтобы модель могла учитывать порядок элементов. После этого данные проходят через `TransformerEncoder`, который состоит из нескольких слоев. Каждый слой включает мультихэдовое внимание, где модель определяет взаимосвязи между временными шагами, и полносвязную сеть для обработки этих взаимосвязей. Слои дополнительно используют механизм нормализации и dropout для стабилизации и регуляризации. После прохождения через энкодер данные усредняются по временной оси, чтобы агрегировать всю информацию в один вектор размерности `d_model`. Этот вектор подается в финальный полносвязный слой, который преобразует его в одно значение — вероятность того, что аудио является DeepFake. Для преобразования вероятности в диапазон [0, 1] используется функция активации Sigmoid.

Код разделен на несколько файлов. В `data/utils.py` содержатся функции для обработки данных: загрузка аудиофайлов, преобразование их в спектрограммы и нормализация, а также функции для работы с `DataLoader`, такие как приведение спектрограмм к фиксированной длине. В `models/transformer.py` описана архитектура модели `TransformerClassifier`, включая входную проекцию, позиционное кодирование, слои трансформера и финальный классификационный слой. В `training/config.py` задаются параметры конфигурации, такие как размер батча, количество эпох, гиперпараметры модели (размерность `d_model`, число голов в механизме внимания, количество слоев, уровень dropout) и пути к данным. В `training/train.py` реализован процесс обучения: загрузка данных, передача их в модель, расчет функции потерь и метрик, а также сохранение лучшей модели на основе F1-метрики. В `inference/predict.py` описан процесс предсказания: аудиофайл загружается, преобразуется в спектрограмму, подается в обученную модель, и результатом становится вероятность DeepFake.